{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pre_training-BERT.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2rSL01d3FRt",
        "colab_type": "text"
      },
      "source": [
        "# PRETRAİNİNG BERT WITH CLOUD TPU\n",
        "\n",
        "---\n",
        "\n",
        "KAYNAK: https://towardsdatascience.com/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S4CiOh3RzFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install sentencepiece\n",
        "!git clone https://github.com/google-research/bert\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import nltk\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import sentencepiece as spm\n",
        "\n",
        "from glob import glob\n",
        "from google.colab import auth, drive\n",
        "from tensorflow.keras.utils import Progbar\n",
        "\n",
        "sys.path.append(\"bert\")\n",
        "\n",
        "from bert import modeling, optimization, tokenization\n",
        "from bert.run_pretraining import input_fn_builder, model_fn_builder\n",
        "\n",
        "auth.authenticate_user()\n",
        "  \n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "# create formatter and add it to the handlers\n",
        "formatter = logging.Formatter('%(asctime)s :  %(message)s')\n",
        "sh = logging.StreamHandler()\n",
        "sh.setLevel(logging.INFO)\n",
        "sh.setFormatter(formatter)\n",
        "log.handlers = [sh]\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  log.info(\"Using TPU runtime\")\n",
        "  USE_TPU = True\n",
        "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "    log.info('TPU address is ' + TPU_ADDRESS)\n",
        "    # Upload credentials to TPU.\n",
        "    with open('/content/adc.json', 'r') as f:\n",
        "      auth_info = json.load(f)\n",
        "    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "    \n",
        "else:\n",
        "  log.warning('Not connected to TPU runtime')\n",
        "  USE_TPU = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPKZjfqM4TZu",
        "colab_type": "text"
      },
      "source": [
        "## VERİ SETİ: \n",
        "\n",
        "---\n",
        "http://opus.nlpl.eu/OpenSubtitles-v2016.php\n",
        "\n",
        "Bu veri setinin 65 farklı dil seçeneği vardır. Veriler satır satır olacak şekildedirler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FotFkkshbdvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AVAILABLE =  {'af','ar','bg','bn','br','bs','ca','cs',\n",
        "              'da','de','el','en','eo','es','et','eu',\n",
        "              'fa','fi','fr','gl','he','hi','hr','hu',\n",
        "              'hy','id','is','it','ja','ka','kk','ko',\n",
        "              'lt','lv','mk','ml','ms','nl','no','pl',\n",
        "              'pt','pt_br','ro','ru','si','sk','sl','sq',\n",
        "              'sr','sv','ta','te','th','tl','tr','uk',\n",
        "              'ur','vi','ze_en','ze_zh','zh','zh_cn',\n",
        "              'zh_en','zh_tw','zh_zh'}\n",
        "\n",
        "LANG_CODE = \"tr\" \n",
        "\n",
        "!wget http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2016/mono/OpenSubtitles.raw.'$LANG_CODE'.gz -O dataset.txt.gz\n",
        "!gzip -d dataset.txt.gz\n",
        "!tail dataset.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDR5Z1MDgB1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CORPUS_SIZE = 700000 # KULLANILACAK OLAN VERİ BOYUTUNU BELİRLEDİK\n",
        "\n",
        "!(head -n $CORPUS_SIZE dataset.txt) > train.txt #EĞİTİM İÇİN KULLANILACAK VERİ SETİ OLUŞTURULDU\n",
        "!(tail -n 200000 dataset.txt) > test.txt #TEST İÇİN KULLANILACAK VERİ SETİ OLUŞTURULDU\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeI-fvf4_B2C",
        "colab_type": "text"
      },
      "source": [
        "## PREPROCESSING ROW DATA:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "1.   harfleri küçük harfe çevirme\n",
        "2.   noktalama işareterini silme\n",
        "3. rakamları silme\n",
        "4. türkçe karakter dönüşümü yapma\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCi2oSdInRkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "regex_tokenizer = nltk.RegexpTokenizer(\"\\w+\")\n",
        "import re\n",
        "\n",
        "def normalize_text(text):\n",
        "  text= text.replace('I','i')\n",
        "  text= text.replace('İ','i')\n",
        "  \n",
        "  # lowercase text\n",
        "  text = str(text).lower()\n",
        "  # remove non-UTF\n",
        "  text = text.encode(\"utf-8\", \"ignore\").decode()\n",
        "  pattern = r\"[{}]\".format('&+#*·–’“•\\'\"?!,.():;><_/-') \n",
        "  text = re.sub(pattern, \" \", text) \n",
        "  \n",
        "  text = re.sub(r'[0-9]+', ' ', text) \n",
        "  text= text.replace('ş','s')\n",
        "  text= text.replace('ı','i')\n",
        "  text= text.replace('ö','o')\n",
        "  text= text.replace('ü','u')\n",
        "  text= text.replace('ğ','g')\n",
        "  text= text.replace('ç','c')\n",
        "  \n",
        "  # remove punktuation symbols\n",
        "  text = \" \".join(regex_tokenizer.tokenize(text))\n",
        "  return text\n",
        "\n",
        "def count_lines(filename):\n",
        "  count = 0\n",
        "  with open(filename) as fi:\n",
        "    for line in fi:\n",
        "      count += 1\n",
        "  return count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gngtEZWqVhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalize_text('İSTANBUL ben geldim. 39 :)))')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_7Q0J9IxCw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalize_text('ISTANBUL ben geldim. 39 :)))')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzT9vSIRxHK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalize_text('iSTANBUL ben geldim. 39 :)))')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_yaztdAxKU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalize_text('ıSTANBUL ben geldim. 39 :)))')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myjxQe5awo1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RAW_DATA_FPATH = \"train.txt\" \n",
        "PRC_DATA_FPATH = \"cleaned_train.txt\"\n",
        "\n",
        "# apply normalization to the dataset\n",
        "total_lines = count_lines(RAW_DATA_FPATH)\n",
        "bar = Progbar(total_lines)\n",
        "\n",
        "with open(RAW_DATA_FPATH,encoding=\"utf-8\") as fi:\n",
        "  with open(PRC_DATA_FPATH, \"w\",encoding=\"utf-8\") as fo:\n",
        "    for l in fi:\n",
        "      fo.write(normalize_text(l)+\"\\n\")\n",
        "      bar.add(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhufF-AGxX7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RAW_DATA_TEST_FPATH = \"test.txt\" \n",
        "PRC_DATA_TEST_FPATH = \"cleaned_test.txt\"\n",
        "\n",
        "# apply normalization to the dataset\n",
        "test_lines = count_lines(RAW_DATA_TEST_FPATH)\n",
        "test_bar = Progbar(test_lines)\n",
        "\n",
        "with open(RAW_DATA_TEST_FPATH,encoding=\"utf-8\") as fi:\n",
        "  with open(PRC_DATA_TEST_FPATH, \"w\",encoding=\"utf-8\") as fo:\n",
        "    for l in fi:\n",
        "      fo.write(normalize_text(l)+\"\\n\")\n",
        "      test_bar.add(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqzeX0Wv_vJg",
        "colab_type": "text"
      },
      "source": [
        "## VOCABULARY OLUŞTURMA:\n",
        "\n",
        "---\n",
        "BERT paper da tokenization için wordPiece tokinizer kullanılmaktadır ama bunun açık kaynak  kodu bulunmuyor, bu yüzden SentencePiece tokinizer kullanıldı(unigram modda). wordpiece tokinizerda kelime eklerinin başına ## geliyorken, sentincepiece de __ gelmektedir. Bu yüzden __ ile başlayan  kelimelerim başına __ yerine ## konulmuştur. placeholder token sayısı, vocabulary'i güncellemek istersek veya trainden sonra fine-tuning için belirtilmiştir, placeholder tokenlar yerine task-specific tokenlar gelecektir. Bu durumda,placeholder tokenlarının yerine yenileri gelecektir ve bu şekilde eğitim öncesi veriler yeniden oluşturulur, model yeni verilere göre ayarlanır.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18nn6eW_s-fV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_PREFIX = \"tokenizer\" \n",
        "VOC_SIZE = 40000\n",
        "SUBSAMPLE_SIZE = 1280000\n",
        "NUM_PLACEHOLDERS = 128\n",
        "\n",
        "SPM_COMMAND = ('--input={} --model_prefix={} '\n",
        "               '--vocab_size={} --input_sentence_size={} '\n",
        "               '--shuffle_input_sentence=true ' \n",
        "               '--bos_id=-1 --eos_id=-1').format(\n",
        "               PRC_DATA_FPATH, MODEL_PREFIX, \n",
        "               VOC_SIZE - NUM_PLACEHOLDERS, SUBSAMPLE_SIZE)\n",
        "\n",
        "spm.SentencePieceTrainer.Train(SPM_COMMAND)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBsURk_h5jw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_sentencepiece_vocab(filepath):\n",
        "  voc = []\n",
        "  with open(filepath, encoding='utf-8') as fi:\n",
        "    for line in fi:\n",
        "      voc.append(line.split(\"\\t\")[0])\n",
        "  # skip the first <unk> token\n",
        "  voc = voc[1:]\n",
        "  return voc\n",
        "\n",
        "snt_vocab = read_sentencepiece_vocab(\"{}.vocab\".format(MODEL_PREFIX))\n",
        "print(\"Learnt vocab size: {}\".format(len(snt_vocab)))\n",
        "print(\"Sample tokens: {}\".format(random.sample(snt_vocab, 10)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QJGFjzOMbfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_sentencepiece_token(token):\n",
        "    if token.startswith(\"▁\"):\n",
        "        return token[1:]\n",
        "    else:\n",
        "        return \"##\" + token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64dcVgD98S28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdTlXDPL8cHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
        "bert_vocab = ctrl_symbols + bert_vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLYSTil4E0Dm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_vocab += [\"[UNUSED_{}]\".format(i) for i in range(VOC_SIZE - len(bert_vocab))]\n",
        "print(len(bert_vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G1jg0cj9Duf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOC_FNAME = \"vocab.txt\" \n",
        "\n",
        "with open(VOC_FNAME, \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quOoRvCpJmcu",
        "colab_type": "text"
      },
      "source": [
        "## PRE-TRAİNİNG VERİ OLUŞTURMA:\n",
        "\n",
        "---\n",
        "Veri BERT modelin eğitimi için uygun hale getiriliyor.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyN1nI04-uKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir ./shards #VERİ SETİ BÜYÜK OLDUĞU İÇİN VERİLERİ SHARDLARA BÖLÜYORUZ, \n",
        "#BU ÖRNEKTE SADECE 100.000 ÖRNEK OLDUĞU İÇİN TEK BİR SHARD OLUŞUYOR\n",
        "!split -a 4 -l 256000 -d $PRC_DATA_FPATH ./shards/shard_\n",
        "!ls ./shards/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnZcD0yIBGPd",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "MAX_SEQ_LENGTH = 128 \n",
        "MASKED_LM_PROB = 0.15\n",
        "MAX_PREDICTIONS = 20 \n",
        "DO_LOWER_CASE = True \n",
        "PROCESSES = 2 \n",
        "PRETRAINING_DIR = \"pretraining_data\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbZjIeVP0T36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#her shard için bertin create_pretraining_data.py, belirlenen parametreler ile çalıştırılıyor\n",
        "XARGS_CMD = (\"ls ./shards/ | \"\n",
        "             \"xargs -n 1 -P {} -I{} \"\n",
        "             \"python3 bert/create_pretraining_data.py \"\n",
        "             \"--input_file=./shards/{} \"\n",
        "             \"--output_file={}/{}.tfrecord \"\n",
        "             \"--vocab_file={} \"\n",
        "             \"--do_lower_case={} \"\n",
        "             \"--max_predictions_per_seq={} \"\n",
        "             \"--max_seq_length={} \"\n",
        "             \"--masked_lm_prob={} \"\n",
        "             \"--random_seed=34 \"\n",
        "             \"--dupe_factor=5\")\n",
        "\n",
        "XARGS_CMD = XARGS_CMD.format(PROCESSES, '{}', '{}', PRETRAINING_DIR, '{}', \n",
        "                             VOC_FNAME, DO_LOWER_CASE, \n",
        "                             MAX_PREDICTIONS, MAX_SEQ_LENGTH, MASKED_LM_PROB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5XzaY8xCdiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.gfile.MkDir(PRETRAINING_DIR)\n",
        "!$XARGS_CMD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rze9XvXJKv8d",
        "colab_type": "text"
      },
      "source": [
        "## DEPOLAMATI AYARLAMA: GOOGLE CLOUD STORAGE\n",
        "\n",
        "---\n",
        "\n",
        "CONSOLE:https://console.cloud.google.com/storage/\n",
        "KULLANILMASININ SEBEBİ VERİ SETİNİN BERTLE EĞİTİMİ SAATLER SÜRDÜĞÜ İÇİN KALDIĞI YERDEN DEVAM ETME, VERİLERİ KAYDETMEK İÇİN OLUŞTURULAN PAKETTE EĞİTİM İÇİN KULLANILANLAR VE ELDE EDİLEN AĞIRLIKLAR SAKLANMAKTADIR, AYRICA SONRAKİ KULLANIM İÇİN DE FAYDALI OLACAKTIR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUtUT3jTjhWE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "project_id ='arctic-analyzer-254614'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8cNGTwUjHaB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gcloud config set project {project_id}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-OinUC97shn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil mb gs://bert_resourse/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDtrt68QQIHs",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "BUCKET_NAME = \"bert_resourse\" \n",
        "MODEL_DIR = \"bert_model\"\n",
        "tf.gfile.MkDir(MODEL_DIR)\n",
        "\n",
        "if not BUCKET_NAME:\n",
        "  log.warning(\"WARNING: BUCKET_NAME is not set. \"\n",
        "              \"You will not be able to train the model.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEpSGpUKReKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use this for BERT-base, HYPER-PARAMETER konfigürasyonu yapılmıştır.\n",
        "\n",
        "bert_base_config = {\n",
        "  \"attention_probs_dropout_prob\": 0.1, \n",
        "  \"directionality\": \"bidi\", \n",
        "  \"hidden_act\": \"gelu\", \n",
        "  \"hidden_dropout_prob\": 0.1, \n",
        "  \"hidden_size\": 768, \n",
        "  \"initializer_range\": 0.02, \n",
        "  \"intermediate_size\": 3072, \n",
        "  \"max_position_embeddings\": 512, \n",
        "  \"num_attention_heads\": 12, \n",
        "  \"num_hidden_layers\": 12, \n",
        "  \"pooler_fc_size\": 768, \n",
        "  \"pooler_num_attention_heads\": 12, \n",
        "  \"pooler_num_fc_layers\": 3, \n",
        "  \"pooler_size_per_head\": 128, \n",
        "  \"pooler_type\": \"first_token_transform\", \n",
        "  \"type_vocab_size\": 2, \n",
        "  \"vocab_size\": VOC_SIZE\n",
        "}\n",
        "\n",
        "with open(\"{}/bert_config.json\".format(MODEL_DIR), \"w\") as fo:\n",
        "  json.dump(bert_base_config, fo, indent=2)\n",
        "  \n",
        "with open(\"{}/{}\".format(MODEL_DIR, VOC_FNAME), \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txgrEDugRG48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if BUCKET_NAME:\n",
        "  !gsutil -m cp -r $MODEL_DIR $PRETRAINING_DIR gs://$BUCKET_NAME"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaIvGU1eMTjr",
        "colab_type": "text"
      },
      "source": [
        "# TRAINING \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXAuzsJfYrio",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "BUCKET_NAME = \"bert_resourse\" \n",
        "MODEL_DIR = \"bert_model\" \n",
        "PRETRAINING_DIR = \"pretraining_data\" \n",
        "VOC_FNAME = \"vocab.txt\" \n",
        "\n",
        "# Input data pipeline config\n",
        "TRAIN_BATCH_SIZE = 128 \n",
        "MAX_PREDICTIONS = 20 \n",
        "MAX_SEQ_LENGTH = 128 \n",
        "MASKED_LM_PROB = 0.15 \n",
        "\n",
        "# Training procedure config\n",
        "EVAL_BATCH_SIZE = 64\n",
        "LEARNING_RATE = 2e-5\n",
        "TRAIN_STEPS = 700000 \n",
        "SAVE_CHECKPOINTS_STEPS = 2500 \n",
        "NUM_TPU_CORES = 8\n",
        "\n",
        "if BUCKET_NAME:\n",
        "  BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
        "else:\n",
        "  BUCKET_PATH = \".\"\n",
        "\n",
        "BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR)\n",
        "DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, PRETRAINING_DIR)\n",
        "\n",
        "VOCAB_FILE = os.path.join(BERT_GCS_DIR, VOC_FNAME)\n",
        "CONFIG_FILE = os.path.join(BERT_GCS_DIR, \"bert_config.json\")\n",
        "\n",
        "INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "\n",
        "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))\n",
        "\n",
        "log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))\n",
        "log.info(\"Using {} data shards\".format(len(input_files)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMahsqUnZ55z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "      bert_config=bert_config,\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=TRAIN_STEPS,\n",
        "      num_warmup_steps=10,\n",
        "      use_tpu=USE_TPU,\n",
        "      use_one_hot_embeddings=True)\n",
        "\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=BERT_GCS_DIR,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=USE_TPU,\n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE)\n",
        "  \n",
        "train_input_fn = input_fn_builder(\n",
        "        input_files=input_files,\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        max_predictions_per_seq=MAX_PREDICTIONS,\n",
        "        is_training=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNt5ykopeIYB",
        "colab_type": "text"
      },
      "source": [
        "Fire!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCuEbr6dv8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}